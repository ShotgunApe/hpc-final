\documentclass[12pt]{article}

\title{HPC Final}
\date{May 4th, 2025}

% author hijacking is so real https://tex.stackexchange.com/questions/63384/add-additional-text-on-title-page
\author{\parbox{\linewidth}{\centering%
    Luna Gal, Will Sieber, Sam Smith, Gus Thomas
	\endgraf\bigskip
	Department of Computer Science, The College of Wooster
	\bigskip
}}

\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{lipsum}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\linespread{.9}\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=6pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\spacing{1.5}

\begin{document}
\maketitle

% Latex can automatically build a table of contents for you
\newpage
\tableofcontents
\newpage

\lstlistoflistings
\newpage

\section{Introduction}
Our initial area of inquiry involved implementing an experiment utilizing Tensor Comprehensions, a domain-specific language and compiler framework developed by Facebook AI Research for streamlining the creation of high-performance machine learning (ML) kernels, particularly for GPUs \cite{tens}. Tensor Comprehensions provided several notable features, including a mathematical syntax inspired by Einstein notation, polyhedral just-in-time (JIT) compilation autotuning, compilation caching, and framework-agnostic integration. However, after beginning our implementation phase, we discovered that Tensor Comprehensions is no longer supported and is no longer available in the Conda repositories. As a result, we were unable to install the necessary tools and subsequently had to pivot our research direction. Hence, our area of focus switched to implementing tensor operations on matrices, such as the Gauss-Jordan reduction method, in both PyTorch and TensorFlow. Using profiling software, we analyze the compute time for both implementations to determine their efficiency.

\section{Related Work}
Previously, Tensor Comprehensions optimized code through linear programming and black box auto-tuning to generate the correctly optimized code \cite{neu}. Using these approaches, this library had the benefit of providing high performant code with no manual intervention. It attempted this by creating a domain specific language to denote the number of possible optimizations that could be made to an operation using a polyhedral model. While other libraries exist from Bondhugula et al. using this same approach, it was the first to combine multiple techniques to optimize tensor operations in a manner that only required the use of high-level code \cite{neu, bondhugula2008practical}. As of 2019, however, this library has since been deprecated.

\section{Methodology}
We implemented the Gauss-Jordan algorithm as a tensor operation to evaluate and compare the compute times of PyTorch and TensorFlow. At its core, the algorithm iterates over each pivot column, checking if the pivot element is zero. If so, the pivot row is swapped with another row containing a non-zero element in that column. Next, to satisfy the requirement that each pivot must be 1, the entire pivot row is divided by the value in the pivot position. Following this, the algorithm eliminates all other entries in the pivot column by subtracting suitable multiples of the pivot row from the other rows, both above and below the pivot. If the matrix is square, this process results in an identity matrix; if the matrix is rectangular, the resulting matrix will contain free variables corresponding to the non-pivot columns.

\section{Implementation}

In order to implement these algorithms, the libraries \textbf{TensorFlow} and \textbf{PyTorch} were used.
\hfill\break

\lstset{style=mystyle}
\lstinputlisting[language=Python,frame=single, caption=Pseudocode for \lstinline{gj(A, B)}]{pseudocode.py}

\section{Data Collection}

\section{Results}

\section{Conclusion and Future Work}

\newpage
% Bibliography
\bibliography{bibliography}
\bibliographystyle{abbrv}

\end{document}
