\documentclass[12pt]{article}

\title{Exploring Tensor Optimizations}
\date{May 4th, 2025}

% author hijacking is so real https://tex.stackexchange.com/questions/63384/add-additional-text-on-title-page
\author{\parbox{\linewidth}{\centering%
    Luna Gal, Will Sieber, Sam Smith, Gus Thomas
	\endgraf\bigskip
	Department of Computer Science, The College of Wooster
	\bigskip
}}

\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{lipsum}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\linespread{.9}\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=6pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\spacing{1.5}

\begin{document}
\maketitle

% Latex can automatically build a table of contents for you
\newpage
\tableofcontents
\newpage

\lstlistoflistings
\newpage

\section{Introduction}
Our initial area of inquiry involved implementing an experiment utilizing Tensor Comprehensions, a domain-specific language and compiler framework developed by Facebook AI Research for streamlining the creation of high-performance machine learning (ML) kernels, particularly for GPUs \cite{tens}. Tensor Comprehensions provided several notable features, including a mathematical syntax inspired by Einstein notation, polyhedral just-in-time (JIT) compilation autotuning, compilation caching, and framework-agnostic integration. However, after beginning our implementation phase, we discovered that Tensor Comprehensions is no longer supported and is no longer available in the Conda repositories. As a result, we were unable to install the necessary tools and subsequently had to pivot our research direction. Hence, our area of focus switched to implementing tensor operations on matrices, such as the Gauss-Jordan reduction method, in both PyTorch and TensorFlow. Using profiling software, we analyze the compute time for both implementations to determine their efficiency.

\section{Related Work}
Previously, Tensor Comprehensions optimized code through linear programming and black box auto-tuning to generate the correctly optimized code \cite{neu}. Using these approaches, this library had the benefit of providing high performant code with no manual intervention. It attempted this by creating a domain specific language to denote the number of possible optimizations that could be made to an operation using a polyhedral model. While other libraries exist from Bondhugula et al. using this same approach, it was the first to combine multiple techniques to optimize tensor operations in a manner that only required the use of high-level code \cite{neu, bondhugula2008practical}. As of 2019, however, this library has been deprecated. In light of this, we opted to simulate what one would experience optimizing tensor operations themselves.

To support our implementation of the Gauss-Jordan algorithm, we consulted a YouTube video as a practical reference for its procedural steps. While academic descriptions of the algorithm are widely available, this video provided a clear demonstration of the row operations involved, including pivot selection, row swapping, and row elimination \cite{gaussjordan_video}.

\section{Methodology}

To accomplish this, we implemented the Gauss-Jordan algorithm as a tensor operation to evaluate and compare the compute times of a naive approach using \textbf{NumPy} and a vectorized approach using \textbf{PyTorch}. At its core, the algorithm iterates over each pivot column, checking if the pivot element is zero. If so, the pivot row is swapped with another row containing a non-zero element in that column. Next, to satisfy the requirement that each pivot must be 1, the entire pivot row is divided by the value in the pivot position. Following this, the algorithm eliminates all other entries in the pivot column by subtracting suitable multiples of the pivot row from the other rows, both above and below the pivot. If the matrix is square and invertible, this process results in an identity matrix; otherwise, the resulting matrix will contain free variables corresponding to the non-pivot columns.

\section{Implementation}

In order to implement these algorithms, the \textbf{PyTorch}, \textbf{NumPy}, and \textbf{CuPy} libraries were used to represent the different levels of optimization. \textbf{Listing 1} provides the implementation using \textbf{PyTorch}:
\hfill\break

\lstset{style=mystyle}
\lstinputlisting[language=Python,frame=single, caption=Code for \lstinline{gj_torch(A, b)}]{pseudocode.py}

We use the cProfile Python library for deterministic profiling of the Python interpreter. Deterministic profilers record the timing intervals of all function calls, function returns, and exception events; this is done via a hook from the Python interpreter loop. Deterministic profilers are contrasted with statistical profilers, wherein statistical profilers take samples of the runtime at known intervals to instrument the execution graph. Deterministic profilers are deterministic since they do not rely on a sampling method, and they return identical statistical results for identical execution graphs. 

Deterministic profilers are usually considered unwieldy, since a large time cost is incurred when synchronizing the profiler's clock with the runtime. However, the Python interpreter loop is so bulky that the cost of deterministic profiling is not considered a hindrance in most applications. Furthermore, deterministic profiles are needed when precise counts of calls are required; this nuance is outside the scope of this paper, since we are not optimizing our kernels by minimizing function calls, but it may interesting to the reader.

\section{Data Collection}
To measure the performance of each algorithm, the \textbf{cProfile} library was used. A wrapper function was written to profile each function with some number \textit{n} iterations. This was then logged to a separate file where execution time was measured and subsequently graphed. This was done manually by copying the execution time from \textbf{*.profile}. \textbf{Listing 2} provides the wrapper function used to profile each library, respectively.
\hfill\break

\lstinputlisting[language=Python,frame=single, caption=Code for \lstinline{compare()}]{pseudocode2.py}

\section{Results}

\section{Conclusion and Future Work}

\newpage
% Bibliography
\bibliography{bibliography}
\bibliographystyle{abbrv}

\end{document}
