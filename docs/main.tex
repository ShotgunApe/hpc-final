\documentclass[12pt]{article}

\title{HPC Final}
\date{May 4th, 2025}

% author hijacking is so real https://tex.stackexchange.com/questions/63384/add-additional-text-on-title-page
\author{\parbox{\linewidth}{\centering%
    Luna Gal, Will Sieber, Sam Smith, Gus Thomas
	\endgraf\bigskip
	Department of Computer Science, The College of Wooster
	\bigskip
}}

\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{lipsum}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\linespread{.9}\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=6pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\spacing{1.5}

\begin{document}
\maketitle

% Latex can automatically build a table of contents for you
\newpage
\tableofcontents
\newpage

\lstlistoflistings
\newpage

\section{Introduction}
Our initial area of inquiry involved implementing an experiment utilizing Tensor Comprehensions, a domain-specific language and compiler framework developed by Facebook AI Research for streamlining the creation of high-performance machine learning (ML) kernels, particularly for GPUs \cite{tens}. Tensor Comprehensions provided several notable features, including a mathematical syntax inspired by Einstein notation, polyhedral just-in-time (JIT) compilation autotuning, compilation caching, and framework-agnostic integration. However, after beginning our implementation phase, we discovered that Tensor Comprehensions is no longer supported and is no longer available in the Conda repositories. As a result, we were unable to install the necessary tools and subsequently had to pivot our research direction. Hence, our area of focus switched to implementing tensor operations on matrices, such as the Gauss-Jordan reduction method, in both PyTorch and TensorFlow. Using profiling software, we analyze the compute time for both implementations to determine their efficiency.

\section{Related Work}
Previously, Tensor Comprehensions optimized code through linear programming and black box auto-tuning to generate the correctly optimized code \cite{neu}. Using these approaches, this library had the benefit of providing high performant code with no manual intervention. It attempted this by creating a domain specific language to denote the number of possible optimizations that could be made to an operation using a polyhedral model. While other libraries exist from Bondhugula et al. using this same approach, it was the first to combine multiple techniques to optimize tensor operations in a manner that only required the use of high-level code \cite{neu, bondhugula2008practical}. As of 2019, however, this library has since been deprecated.

\section{Methodology}


\section{Implementation}

In order to implement these algorithms, the libraries \textbf{TensorFlow} and \textbf{PyTorch} were used.
\hfill\break

\lstset{style=mystyle}
\lstinputlisting[language=Python,frame=single, caption=Pseudocode for \lstinline{gj(A, B)}]{pseudocode.py}

\section{Data Collection}

\section{Results}

\section{Conclusion and Future Work}

\newpage
% Bibliography
\bibliography{bibliography}
\bibliographystyle{abbrv}

\end{document}
